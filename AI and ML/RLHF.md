# Reinforcement Learning from Human Feedback
- Use human feedback to help ML models to self-learn more efficiently.
- RLHF incorporated human feedback in the reward function, to be more aligned with human goals, wants and needs/
- RLHF is used throughout GenAI apps including LLM models.
- IT significantly enhances the model performance.

## How this works?
- Data Collection
- Supervised fine-tuning of a language model
- Build a separate reward model
- Optimize the language model with the reward-based model