# Automatic Evaluation
- Evaluate a model for quality control
- Built-in task types:
    - Text summarization
    - question and answer
    - text classification
    - open-ended text generation…
- Bring your own prompt dataset or use built-in curated prompt datasets
- Scores are calculated automatically
- Model scores are calculated using various statistical methods (e.g. BERTScore, F1…)

## Benchmark Datasets
-  Curated collections of data designed specifically at evaluating the performance of language models
- Wide range of topics, complexities, linguistic phenomena
- Helpful to measure: accuracy, speed and efficiency, scalability
- Some benchmarks datasets allow you to very quickly detect any kind of bias and potential discrimination against a group of people
- You can also create your own benchmark dataset that is specific to your business

# Human Evaluation

## Automated Metrics to Evaluate an FM
- ROUGE
- BLEU
- BERTScore
- Perplexity

## Business Metrics to Evaluate a Model on
- User Satisfaction