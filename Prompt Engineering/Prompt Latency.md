# Prompt Latency
- Latency is how fast the model responds
- Itâ€™s impacted by a few parameters:
    - The model size
    - The model type itself (Llama has a different performance than Claude)
    - The number of tokens in the input (the bigger the slower)
    - The number of tokens in the output (the bigger the slower)
- Latency is not impacted by Top P, Top K, Temperature